# -*- coding: utf-8 -*-
"""Loan_approval_model_b.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Gn1pJeINKG2sODNy1JeRxxmQQKAO0lZ
"""

! git clone https://github.com/wlifferth/ghw-2023-02

import pandas as pd

df = pd.read_csv('ghw-2023-02/loan_applications.csv')
df.head()

from sklearn.model_selection import train_test_split
df['is_creditworthy'] = df['will_default'] == False

X_train, X_test, y_train, y_test = train_test_split(df[['credit_score', 'race', 'zipcode']], df['is_creditworthy'], test_size = 0.3, random_state=42)

len(X_train)

len(X_test)

from sklearn.linear_model import LogisticRegression

X_train_vec = X_train[['credit_score']]
X_test_vec = X_test[['credit_score']]

basic_model = LogisticRegression()

basic_model.fit(X_train_vec, y_train)

basic_model.score(X_test_vec, y_test)

basic_model.coef_

basic_model.intercept_

basic_model_test_predictions = basic_model.predict(X_test_vec)

def evaluate_by_race(input_data, predictions, correct_answer):
  for race in sorted(input_data['race'].unique()):
    print(race)
    indices = input_data['race'] == race
    preds_by_race = predictions[indices]
    correct_answer_by_race = correct_answer[indices]
    print(f'\tAccuracy: {sum(preds_by_race == correct_answer_by_race) / len(preds_by_race)}')

evaluate_by_race(X_test, basic_model_test_predictions, y_test)

"""**The Economics of Being a Bank**

Average Home Loan: 500k

Profit from a Loan Being Paid Back: 100k

Loss from a Loan being Defaulted on: 500k

Conclusion: Giving a loan to someone who will default is 5x worse than not giving a loan to someone who will pay it back
"""

max_profit_model = LogisticRegression(class_weight={1:1, 0:5})
max_profit_model.fit(X_train_vec, y_train)
max_profit_model.score(X_test_vec, y_test)

max_profit_predictions = max_profit_model.predict(X_test_vec)

sum(y_test == True)
sum(max_profit_predictions == y_test)

sum((max_profit_predictions == y_test) & (y_test == True)) / sum(y_test == True)

def get_confusion_matrix(predictions, correct_answer):
  true_positive_rate = sum((predictions == correct_answer) & (correct_answer == True)) / sum(correct_answer == True)
  true_negative_rate = sum((predictions == correct_answer) & (correct_answer == False)) / sum(correct_answer == False)
  print("True Positive Rate: ", true_positive_rate)
  print("True Negative Rate: ", true_negative_rate)

print("Basic Model")
get_confusion_matrix(basic_model_test_predictions, y_test)

print("Max Profit Model")
get_confusion_matrix(max_profit_predictions, y_test)

"""## Trying to improve the model by adding more data"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

ct = ColumnTransformer([('one-hot-encoder', OneHotEncoder(), ['race', 'zipcode'])], remainder = 'passthrough')
X_train_vec_full = ct.fit_transform(X_train)
X_test_vec_full = ct.transform(X_test)

full_model = LogisticRegression(class_weight = {1:1, 0:5})
full_model.fit(X_train_vec_full, y_train)
full_model.score(X_test_vec_full, y_test)
full_model_predictions = full_model.predict(X_test_vec_full)

def evaluate_by_race(input_data, predictions, correct_answer):
  print(f'\tAccuracy: {sum(predictions == correct_answer) / len(predictions)}')  
  true_positive_rate = sum((predictions == correct_answer) & (correct_answer == True)) / sum(correct_answer == True)
  true_negative_rate = sum((predictions == correct_answer) & (correct_answer == False)) / sum(correct_answer == False)
  print(f'\tTPR: {true_positive_rate}')
  print(f'\tTNR: {true_negative_rate}')
  f1_values = []
  for race in sorted(input_data['race'].unique()):
    print(race)
    indices = input_data['race'] == race
    preds_by_race = predictions[indices]
    correct_answer_by_race = correct_answer[indices]
    print(f'\tAccuracy: {sum(preds_by_race == correct_answer_by_race) / len(preds_by_race)}')  
    true_positive_rate = sum((preds_by_race == correct_answer_by_race) & (correct_answer_by_race == True)) / sum(correct_answer_by_race == True)
    true_negative_rate = sum((preds_by_race == correct_answer_by_race) & (correct_answer_by_race == False)) / sum(correct_answer_by_race == False)
    f1_approximate = true_positive_rate * true_negative_rate
    f1_values.append(f1_approximate)
    print(f'\tTPR: {true_positive_rate}')
    print(f'\tTNR: {true_negative_rate}')
    print(f'\tf1_approximate: {f1_approximate}')

  min_f1 = min(f1_values)
  max_f1 = max(f1_values)
  print(f'min f1: {min_f1}')
  print(f'max f1: {max_f1}')
  print(f'f1 spread: {max_f1 - min_f1}')

evaluate_by_race(X_test, full_model_predictions, y_test)

"""This indicates that there are biases on people who are getting loans, based on their backgrounds. 

Hence, it is important to make our model more fairer.
"""

df['race'].value_counts() / len(df['race'])

X_train['race'].value_counts()

#We're going to try down-sampling our data for more fairness
number_from_each_category = 616
selected_rows = []

X_train['is_creditworthy'] = y_train
for race in sorted(X_train['race'].unique()):
  selected = X_train[X_train['race'] == race][:number_from_each_category]
  selected_rows.append(selected)

X_train = pd.concat(selected_rows)
X_train.head()

X_train['race'].value_counts()

y_train = X_train['is_creditworthy']
X_train = X_train.drop('is_creditworthy', axis=1)

X_train_vec = ct.fit_transform(X_train)
X_test_vec = ct.transform(X_test)
rebalanced_model = LogisticRegression(class_weight={1:1, 0:5})
rebalanced_model.fit(X_train_vec, y_train)
rebalanced_model.score(X_test_vec, y_test)

rebalanced_predictions = rebalanced_model.predict(X_test_vec)
evaluate_by_race(X_test, rebalanced_predictions, y_test)

